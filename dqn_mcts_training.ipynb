{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment lux_ai_s2 failed: No module named 'vec_noise'\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import make\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from Scripts.MCTS import agent_mcts, MCTS\n",
    "from Scripts.Deep_Q_Learning import DQN\n",
    "import matplotlib.pyplot as plt\n",
    "from Scripts.training import train_agent,load_agent, save_agent\n",
    "from Scripts.test_model import test_agent\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Initialisating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 7\n",
    "rows = 6\n",
    "env = make(\"connectx\", configuration={\"rows\":rows, \"columns\":cols})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL_agent1 = False\n",
    "\n",
    "PATH_TO_LOAD = \"models/\"\n",
    "NAME_AGENT_1 = \"1v2_ep20.pt\"\n",
    "\n",
    "\n",
    "if(LOAD_MODEL_agent1):\n",
    "    agent1 = load_agent(PATH_TO_LOAD+NAME_AGENT_1)\n",
    "else :\n",
    "    agent1 = DQN()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Training against MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_STEPS = 1\n",
    "EPOCHS = 100\n",
    "\n",
    "FOLLOW_TRAINING = False\n",
    "NB_GAMES_TEST = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s][W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "/Users/adivanovic/Desktop/X/X-3A/INF581/connect4_rl/Scripts/Deep_Q_Learning.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(observation).float().detach()\n",
      "/Users/adivanovic/Desktop/X/X-3A/INF581/connect4_rl/Scripts/training.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 Average Reward -0.1 Last Reward -1 Epsilon 0.9993002099650037\n",
      "Episode 20 Average Reward -0.05 Last Reward -1 Epsilon 0.9909408287818039\n",
      "Episode 30 Average Reward -0.03333333333333333 Last Reward -1 Epsilon 0.9802957226154846\n",
      "Episode 40 Average Reward -0.025 Last Reward -1 Epsilon 0.9695710273926083\n",
      "Episode 50 Average Reward -0.02 Last Reward -1 Epsilon 0.9589636631801307\n",
      "Episode 60 Average Reward -0.016666666666666666 Last Reward -1 Epsilon 0.948472346345676\n",
      "Episode 70 Average Reward -0.014285714285714285 Last Reward -1 Epsilon 0.9380958073001479\n",
      "Episode 80 Average Reward -0.0125 Last Reward -1 Epsilon 0.9278327903440918\n",
      "Episode 90 Average Reward -0.011111111111111112 Last Reward -1 Epsilon 0.9176820535157378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [03:47<00:00, 227.36s/it]\n"
     ]
    }
   ],
   "source": [
    "history_random = []\n",
    "history_MCTS = []\n",
    "history_negamax = []\n",
    "\n",
    "for i in tqdm(range(NB_STEPS)):\n",
    "    agent1 = train_agent(env,agent1,agent_mcts,n_player=1,epochs=EPOCHS,display_info=True)\n",
    "\n",
    "    if FOLLOW_TRAINING:\n",
    "        current_history_random = test_agent(env,agent1,\"random\",n_player=1,nb_games=NB_GAMES_TEST)\n",
    "        current_history_MCTS = test_agent(env,agent1,agent_mcts,n_player=1,nb_games=NB_GAMES_TEST)\n",
    "        current_history_negamax = test_agent(env,agent1,\"negamax\",n_player =1, nb_games=NB_GAMES_TEST)\n",
    "    \n",
    "        history_random.append(current_history_random[1]/NB_GAMES_TEST*100)\n",
    "        history_MCTS.append(current_history_MCTS[1]/NB_GAMES_TEST*100)\n",
    "        history_negamax.append(current_history_negamax[1]/NB_GAMES_TEST*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Testing Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_GAMES_TEST = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Against MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0, -1: 10, None: 0, 0: 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_MCTS = test_agent(env,agent1,agent_mcts,n_player=1,nb_games=NB_GAMES_TEST)\n",
    "history_MCTS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Against RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 9, -1: 1, None: 0, 0: 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_random = test_agent(env,agent1,\"random\",n_player=1,nb_games=NB_GAMES_TEST)\n",
    "history_random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Against NEGAMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0, -1: 10, None: 0, 0: 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_negamax = test_agent(env,agent1,\"negamax\",n_player =1, nb_games=NB_GAMES_TEST)\n",
    "history_negamax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Saving agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = True\n",
    "PATH_TO_SAVE = \"models/\"\n",
    "\n",
    "NAME1 = \"1vrandom_ep\"+ str(NB_STEPS*EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE :\n",
    "    save_agent(agent1, path_to_save=PATH_TO_SAVE, name=NAME1, epochs=NB_STEPS*EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
