{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment lux_ai_s2 failed: No module named 'vec_noise'\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import make\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from Scripts.MCTS import agent_mcts, MCTS\n",
    "from Scripts.Deep_Q_Learning import DQN\n",
    "import matplotlib.pyplot as plt\n",
    "from Scripts.training import train_agent,load_agent\n",
    "from Scripts.test_model import test_agent\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Initialisating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 7\n",
    "rows = 6\n",
    "env = make(\"connectx\", configuration={\"rows\":rows, \"columns\":cols})\n",
    "SAVE_MODEL = True\n",
    "LOAD_MODEL_agent1 = False\n",
    "LOAD_MODEL_agent2 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(LOAD_MODEL_agent1):\n",
    "    PATH_TO_LOAD = \"\"\n",
    "    agent1 = load_agent(PATH_TO_LOAD)\n",
    "if(LOAD_MODEL_agent2):\n",
    "    PATH_TO_LOAD = \"\"\n",
    "    agent2 = load_agent(PATH_TO_LOAD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Training against MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_MCTS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adivanovic/Desktop/X/X-3A/INF581/connect4_rl/Scripts/Deep_Q_Learning.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(observation).float().detach()\n",
      "/Users/adivanovic/Desktop/X/X-3A/INF581/connect4_rl/Scripts/training.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 Average Reward -0.1 Last Reward -1 Epsilon 0.9902473782645512\n",
      "Episode 20 Average Reward -0.05 Last Reward -1 Epsilon 0.9755037838121503\n",
      "Episode 30 Average Reward -0.03333333333333333 Last Reward -1 Epsilon 0.9581009410296466\n",
      "Episode 40 Average Reward -0.025 Last Reward -1 Epsilon 0.9441191758589251\n",
      "Episode 50 Average Reward -0.02 Last Reward -1 Epsilon 0.9308998497723158\n",
      "Episode 60 Average Reward -0.016666666666666666 Last Reward -1 Epsilon 0.919243517949935\n",
      "Episode 70 Average Reward -0.014285714285714285 Last Reward -1 Epsilon 0.9053759833602809\n",
      "Episode 80 Average Reward -0.0125 Last Reward -1 Epsilon 0.8907372526663989\n",
      "Episode 90 Average Reward -0.011111111111111112 Last Reward -1 Epsilon 0.8754592698971296\n",
      "Episode 100 Average Reward -0.01 Last Reward -1 Epsilon 0.8605293890119464\n",
      "Episode 110 Average Reward -0.00909090909090909 Last Reward -1 Epsilon 0.846785116915467\n",
      "Episode 120 Average Reward -0.008333333333333333 Last Reward -1 Epsilon 0.8325107322566739\n",
      "Episode 130 Average Reward -0.007692307692307693 Last Reward -1 Epsilon 0.818395125359522\n",
      "Episode 140 Average Reward -0.007142857142857143 Last Reward -1 Epsilon 0.8037950777359838\n",
      "Episode 150 Average Reward -0.006666666666666667 Last Reward -1 Epsilon 0.7894554927960618\n",
      "Episode 160 Average Reward -0.00625 Last Reward -1 Epsilon 0.7752166573330537\n",
      "Episode 170 Average Reward -0.0058823529411764705 Last Reward -1 Epsilon 0.7639037603353596\n",
      "Episode 180 Average Reward 0.005555555555555556 Last Reward 1 Epsilon 0.7493010570006634\n",
      "Episode 190 Average Reward -0.005263157894736842 Last Reward -1 Epsilon 0.7363753520758766\n",
      "Episode 200 Average Reward -0.005 Last Reward -1 Epsilon 0.7234555399987465\n",
      "Episode 210 Average Reward -0.004761904761904762 Last Reward -1 Epsilon 0.7124703718089022\n",
      "Episode 220 Average Reward 0.004545454545454545 Last Reward 1 Epsilon 0.7014415311894581\n",
      "Episode 230 Average Reward -0.004347826086956522 Last Reward -1 Epsilon 0.6894793087666956\n",
      "Episode 240 Average Reward -0.004166666666666667 Last Reward -1 Epsilon 0.6772468248683754\n",
      "Episode 250 Average Reward 0.004 Last Reward 1 Epsilon 0.6650318156688173\n",
      "Episode 260 Average Reward -0.0038461538461538464 Last Reward -1 Epsilon 0.65297181600833\n",
      "Episode 270 Average Reward -0.003703703703703704 Last Reward -1 Epsilon 0.642221421484558\n",
      "Episode 280 Average Reward -0.0035714285714285713 Last Reward -1 Epsilon 0.6351953487882008\n",
      "Episode 290 Average Reward -0.0034482758620689655 Last Reward -1 Epsilon 0.6282461431907643\n",
      "Episode 300 Average Reward -0.0033333333333333335 Last Reward -1 Epsilon 0.6213729637457991\n",
      "Episode 310 Average Reward -0.0032258064516129032 Last Reward -1 Epsilon 0.6145749787070303\n",
      "Episode 320 Average Reward -0.003125 Last Reward -1 Epsilon 0.6078513654277099\n",
      "Episode 330 Average Reward -0.0030303030303030303 Last Reward -1 Epsilon 0.6012013102610619\n",
      "Episode 340 Average Reward -0.0029411764705882353 Last Reward -1 Epsilon 0.5946240084618244\n",
      "Episode 350 Average Reward -0.002857142857142857 Last Reward -1 Epsilon 0.5881186640888593\n",
      "Episode 360 Average Reward -0.002777777777777778 Last Reward -1 Epsilon 0.5821500469770029\n",
      "Episode 370 Average Reward -0.002702702702702703 Last Reward -1 Epsilon 0.5769339434210992\n",
      "Episode 380 Average Reward -0.002631578947368421 Last Reward -1 Epsilon 0.5717645765037085\n",
      "Episode 390 Average Reward -0.002564102564102564 Last Reward -1 Epsilon 0.5661317540300521\n",
      "Episode 400 Average Reward -0.0025 Last Reward -1 Epsilon 0.5601621537319724\n",
      "Episode 410 Average Reward -0.0024390243902439024 Last Reward -1 Epsilon 0.5541446545800192\n",
      "Episode 420 Average Reward -0.002380952380952381 Last Reward -1 Epsilon 0.5488500558998584\n",
      "Episode 430 Average Reward -0.002325581395348837 Last Reward -1 Epsilon 0.5439323225025948\n",
      "Episode 440 Average Reward -0.0022727272727272726 Last Reward -1 Epsilon 0.5374438190882207\n",
      "Episode 450 Average Reward -0.0022222222222222222 Last Reward -1 Epsilon 0.5316703699109782\n",
      "Episode 460 Average Reward -0.002173913043478261 Last Reward -1 Epsilon 0.5253281378578852\n",
      "Episode 470 Average Reward -0.002127659574468085 Last Reward -1 Epsilon 0.5190615615300151\n",
      "Episode 480 Average Reward -0.0020833333333333333 Last Reward -1 Epsilon 0.5128697384392993\n",
      "Episode 490 Average Reward -0.0020408163265306124 Last Reward -1 Epsilon 0.5067517768633404\n",
      "Episode 500 Average Reward -0.002 Last Reward -1 Epsilon 0.5007067957169908\n",
      "Episode 510 Average Reward -0.00196078431372549 Last Reward -1 Epsilon 0.49473392442546177\n",
      "Episode 520 Average Reward -0.0019230769230769232 Last Reward -1 Epsilon 0.48883230279894735\n",
      "Episode 530 Average Reward -0.0018867924528301887 Last Reward -1 Epsilon 0.48300108090874144\n",
      "Episode 540 Average Reward -0.001851851851851852 Last Reward -1 Epsilon 0.4772394189648366\n",
      "Episode 550 Average Reward -0.0018181818181818182 Last Reward -1 Epsilon 0.4715464871949787\n",
      "Episode 560 Average Reward -0.0017857142857142857 Last Reward -1 Epsilon 0.46601466399782127\n",
      "Episode 570 Average Reward -0.0017543859649122807 Last Reward -1 Epsilon 0.4609163399661372\n",
      "Episode 580 Average Reward -0.0017241379310344827 Last Reward -1 Epsilon 0.45587379295165886\n",
      "Episode 590 Average Reward -0.001694915254237288 Last Reward -1 Epsilon 0.450886412739024\n",
      "Episode 600 Average Reward -0.0016666666666666668 Last Reward -1 Epsilon 0.4459535957887874\n",
      "Episode 610 Average Reward -0.001639344262295082 Last Reward -1 Epsilon 0.44085425189486804\n",
      "Episode 620 Average Reward -0.0016129032258064516 Last Reward -1 Epsilon 0.43559535441751174\n",
      "Episode 630 Average Reward -0.0015873015873015873 Last Reward -1 Epsilon 0.4303991896972027\n",
      "Episode 640 Average Reward -0.0015625 Last Reward -1 Epsilon 0.4252650094024088\n",
      "Episode 650 Average Reward -0.0015384615384615385 Last Reward -1 Epsilon 0.42019207412835496\n",
      "Episode 660 Average Reward -0.0015151515151515152 Last Reward -1 Epsilon 0.4151796532905364\n",
      "Episode 670 Average Reward -0.0014925373134328358 Last Reward -1 Epsilon 0.41022702501950387\n",
      "Episode 680 Average Reward -0.0014705882352941176 Last Reward -1 Epsilon 0.40533347605690256\n",
      "Episode 690 Average Reward -0.0014492753623188406 Last Reward -1 Epsilon 0.4004983016527502\n",
      "Episode 700 Average Reward -0.0014285714285714286 Last Reward -1 Epsilon 0.3957208054639432\n",
      "Episode 710 Average Reward -0.0014084507042253522 Last Reward -1 Epsilon 0.3910002994539706\n",
      "Episode 720 Average Reward -0.001388888888888889 Last Reward -1 Epsilon 0.3863361037938264\n",
      "Episode 730 Average Reward -0.0013698630136986301 Last Reward -1 Epsilon 0.3817275467641039\n",
      "Episode 740 Average Reward -0.0013513513513513514 Last Reward -1 Epsilon 0.37721168582683906\n",
      "Episode 750 Average Reward -0.0013333333333333333 Last Reward -1 Epsilon 0.3729356783584225\n",
      "Episode 760 Average Reward -0.0013157894736842105 Last Reward -1 Epsilon 0.3688556631182666\n",
      "Episode 770 Average Reward -0.0012987012987012987 Last Reward -1 Epsilon 0.3647838022986049\n",
      "Episode 780 Average Reward -0.001282051282051282 Last Reward -1 Epsilon 0.3604323400875834\n",
      "Episode 790 Average Reward -0.0012658227848101266 Last Reward -1 Epsilon 0.3561327859471909\n",
      "Episode 800 Average Reward -0.00125 Last Reward -1 Epsilon 0.35216615500948345\n",
      "Episode 810 Average Reward -0.0012345679012345679 Last Reward -1 Epsilon 0.34831336386375455\n",
      "Episode 820 Average Reward -0.0012195121951219512 Last Reward -1 Epsilon 0.3445027232750893\n",
      "Episode 830 Average Reward -0.0012048192771084338 Last Reward -1 Epsilon 0.34049533000690735\n",
      "Episode 840 Average Reward -0.0011904761904761906 Last Reward -1 Epsilon 0.3364336020677336\n",
      "Episode 850 Average Reward -0.001176470588235294 Last Reward -1 Epsilon 0.3324203259938218\n",
      "Episode 860 Average Reward -0.0011627906976744186 Last Reward -1 Epsilon 0.32845492380868474\n",
      "Episode 870 Average Reward -0.0011494252873563218 Last Reward -1 Epsilon 0.3245368244304469\n",
      "Episode 880 Average Reward -0.0011363636363636363 Last Reward -1 Epsilon 0.3206654635896002\n",
      "Episode 890 Average Reward -0.0011235955056179776 Last Reward -1 Epsilon 0.31684028374773965\n",
      "Episode 900 Average Reward -0.0011111111111111111 Last Reward -1 Epsilon 0.3130607340172694\n",
      "Episode 910 Average Reward -0.001098901098901099 Last Reward -1 Epsilon 0.3093262700820653\n",
      "Episode 920 Average Reward -0.0010869565217391304 Last Reward -1 Epsilon 0.30563635411908485\n",
      "Episode 930 Average Reward -0.001075268817204301 Last Reward -1 Epsilon 0.30199045472091235\n",
      "Episode 940 Average Reward -0.0010638297872340426 Last Reward -1 Epsilon 0.2983880468192274\n",
      "Episode 950 Average Reward -0.0010526315789473684 Last Reward -1 Epsilon 0.2948286116091861\n",
      "Episode 960 Average Reward -0.0010416666666666667 Last Reward -1 Epsilon 0.2915739480858505\n",
      "Episode 970 Average Reward -0.0010309278350515464 Last Reward -1 Epsilon 0.28838405175557963\n",
      "Episode 980 Average Reward -0.0010204081632653062 Last Reward -1 Epsilon 0.28522905373726226\n",
      "Episode 990 Average Reward -0.00101010101010101 Last Reward -1 Epsilon 0.28210857223410923\n"
     ]
    }
   ],
   "source": [
    "agent1 = DQN()\n",
    "agent1 = train_agent(env,agent1,agent_mcts,n_player=1,epoch=TRAINING_EPOCHS,display_info=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test against MCTS and orther agents to validate training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_GAMES_TEST = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing against MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_history_MCTS = test_agent(env,agent1,agent_mcts,n_player=1,nb_games=NB_GAMES_TEST)\n",
    "current_history_MCTS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing against RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 84, -1: 16, None: 0, 0: 0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_history_rd = test_agent(env,agent1,\"random\",n_player=1,nb_games=NB_GAMES_TEST)\n",
    "current_history_rd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing against NEGAMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 84, -1: 16, None: 0, 0: 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_history_negamax = test_agent(env,agent1,\"negamax\",n_player=1,nb_games=NB_GAMES_TEST)\n",
    "current_history_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
