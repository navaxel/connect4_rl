{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import make\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "\n",
    "from Scripts.MCTS import agent_mcts\n",
    "from Scripts.Deep_Q_Learning import DQN\n",
    "import matplotlib.pyplot as plt\n",
    "from Scripts.training import train_agent,load_agent\n",
    "from Scripts.test_model import test_agent\n",
    "from Scripts.training import save_agent\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Initialisating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 7\n",
    "rows = 6\n",
    "env = make(\"connectx\", configuration={\"rows\":rows, \"columns\":cols})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL_agent1 = False\n",
    "\n",
    "PATH_TO_LOAD = \"models/\"\n",
    "NAME_AGENT_1 = \"\"\n",
    "\n",
    "\n",
    "if(LOAD_MODEL_agent1):\n",
    "    agent1 = load_agent(PATH_TO_LOAD+NAME_AGENT_1)\n",
    "else :\n",
    "    agent1 = DQN()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Training against Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_STEPS = 1\n",
    "EPOCHS = 1000\n",
    "\n",
    "FOLLOW_TRAINING = False\n",
    "NB_GAMES_TEST = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m history_negamax \u001b[39m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NB_STEPS):\n\u001b[0;32m----> 6\u001b[0m     agent1 \u001b[39m=\u001b[39m train_agent(env,agent1,\u001b[39m\"\u001b[39;49m\u001b[39mrandom\u001b[39;49m\u001b[39m\"\u001b[39;49m,n_player\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,epochs\u001b[39m=\u001b[39;49mEPOCHS,display_info\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      8\u001b[0m     \u001b[39mif\u001b[39;00m FOLLOW_TRAINING:\n\u001b[1;32m      9\u001b[0m         current_history_random \u001b[39m=\u001b[39m test_agent(env,agent1,\u001b[39m\"\u001b[39m\u001b[39mrandom\u001b[39m\u001b[39m\"\u001b[39m,n_player\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,nb_games\u001b[39m=\u001b[39mNB_GAMES_TEST)\n",
      "File \u001b[0;32m~/Documents/3A/P2/INF581/connect4_rl/Scripts/training.py:35\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(env, agent_to_train, agent_to_play_against, n_player, epochs, rows, cols, sync_freq, display_info, save, path_to_save, name)\u001b[0m\n\u001b[1;32m     33\u001b[0m exp \u001b[39m=\u001b[39m (state, action, reward, state_, done)\n\u001b[1;32m     34\u001b[0m agent_to_train\u001b[39m.\u001b[39mreplay\u001b[39m.\u001b[39madd(exp)\n\u001b[0;32m---> 35\u001b[0m agent_to_train\u001b[39m.\u001b[39;49mlearn()\n\u001b[1;32m     36\u001b[0m state \u001b[39m=\u001b[39m state_\n\u001b[1;32m     37\u001b[0m score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "File \u001b[0;32m~/Documents/3A/P2/INF581/connect4_rl/Scripts/Deep_Q_Learning.py:173\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39m### END SOLUTION ###\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[1;32m    171\u001b[0m \u001b[39m# Complute ð›Q\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 173\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    174\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    176\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexploration_rate \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m EXPLORATION_DECAY\n",
      "File \u001b[0;32m~/miniconda3/envs/connect4/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/connect4/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_random = []\n",
    "history_MCTS = []\n",
    "history_negamax = []\n",
    "\n",
    "for i in range(NB_STEPS):\n",
    "    agent1 = train_agent(env,agent1,\"random\",n_player=1,epochs=EPOCHS,display_info=False)\n",
    "\n",
    "    if FOLLOW_TRAINING:\n",
    "        current_history_random = test_agent(env,agent1,\"random\",n_player=1,nb_games=NB_GAMES_TEST)\n",
    "        current_history_MCTS = test_agent(env,agent1,agent_mcts,n_player=1,nb_games=NB_GAMES_TEST)\n",
    "        current_history_negamax = test_agent(env,agent1,\"negamax\",n_player =1, nb_games=NB_GAMES_TEST)\n",
    "    \n",
    "        history_random.append(current_history_random[1]/NB_GAMES_TEST*100)\n",
    "        history_MCTS.append(current_history_MCTS[1]/NB_GAMES_TEST*100)\n",
    "        history_negamax.append(current_history_negamax[1]/NB_GAMES_TEST*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Testing Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_GAMES_TEST = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Against random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_random = test_agent(env,agent1,\"random\",n_player=1,nb_games=NB_GAMES_TEST)\n",
    "plt.plot(history_random)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Against MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_MCTS = test_agent(env,agent1,agent_mcts,n_player=1,nb_games=NB_GAMES_TEST)\n",
    "plt.plot(history_MCTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Against MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_negamax = test_agent(env,agent1,\"negamax\",n_player =1, nb_games=NB_GAMES_TEST)\n",
    "plt.plot(history_negamax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Saving Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = True\n",
    "PATH_TO_SAVE = \"models_random/\"\n",
    "\n",
    "NAME1 = \"1vrandom_ep\"+ str(NB_STEPS*RANDOM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SAVE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mif\u001b[39;00m SAVE :\n\u001b[1;32m      2\u001b[0m     save_agent(agent1, path_to_save\u001b[39m=\u001b[39mPATH_TO_SAVE, name\u001b[39m=\u001b[39mNAME1, epochs\u001b[39m=\u001b[39mNB_SWITCH\u001b[39m*\u001b[39mADVERSARIAL_EPOCHS)\n\u001b[1;32m      3\u001b[0m     save_agent(agent2, path_to_save\u001b[39m=\u001b[39mPATH_TO_SAVE, name\u001b[39m=\u001b[39mNAME2, epochs\u001b[39m=\u001b[39mNB_SWITCH\u001b[39m*\u001b[39mADVERSARIAL_EPOCHS)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SAVE' is not defined"
     ]
    }
   ],
   "source": [
    "if SAVE :\n",
    "    save_agent(agent1, path_to_save=PATH_TO_SAVE, name=NAME1, epochs=NB_STEPS*RANDOM_EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connect4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa538ee7a8bce147886b12d01d982610e750a6b3919ce2e6e671e2840062cc59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
